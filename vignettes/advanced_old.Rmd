---
title: "Advanced usage"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Advanced usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(bayesplay)
```

## Computing posteriors

The `bayesplay` package also includes functionality for computing basic posteriors. 

To calculate a posterior, two things are needed: 

1. A **prior**, that assigns probabilities to parameter values. This prior can be thought of as beliefs one holds about parameters values before the data have been seen. 

2. A **likelihood**, that provides **evidence**, obtained from the data, for different parameter values. 

From these two ingredients it is possible to calculate a **posterior**, that represents our new beliefs about parameter values after the **evidence** from the data has been taken into account. 

First, we’ll define a prior:

```{r}
prior_model <- prior(distribution = "normal", mean = 0, sd = 13.3, range = c(0, Inf))
plot(prior_model)
```

Next, we’ll define the likelihood:

```{r}
data_model <- likelihood(distribution = "normal", mean = 5.5, sd = 32.35)
plot(data_model)
```

To calculate the posterior, we first multiple the **prior** by the **likelihood**. That is, we *weight* the **evidence** for each parameter value (obtained from the data and represented by the likelihood) by the our **prior**. 

```{r}
weighted_likelihood <- data_model * prior_model
plot(weighted_likelihood)
```


The result of this calculation, while proportional to the posterior is itself, not a posterior distribution. To produce a posterior distribution, the final step is to divide the output from the product of the data model and the prior by a normalising constant. To do this with the `bayesplay` package it is only necessary to set `type` to `posterior` in the call to the plot function.

```{r}
plot(weighted_likelihood, type = "posterior")
```

In order to see how *beliefs* about parameter values have been updated in response to *evidence* from the data it is helpful to plot the *prior* and the *posterior* on the same figure. To do this with the `bayesplay` package one just needs to set the `type` to `pp` (short for *prior and posterior*). 


```{r}
plot(data_model * prior_model, type = "pp")
```

### Computing Bayes factors from a prior and posterior 

With a prior and a posterior in hand we can examine the density of both distributions at various values of the parameter. For example, we might examine the density of the prior and posterior distributions at the value 0. The ratio of these two values (known as the Savage-Dicky density ratio) represents the change in our belief about the proposition that $\theta$ = 0.

It is possible to do this directly using `prior_function` and `posterior_function` from the prior object and the product of the prior and likelihood objects. 

```{r} 
posterior <- data_model * prior_model
prior_model$prior_function(0) / posterior$posterior_function(0) 
``` 

However, an alternative, and recommended method, is instead to use the `sd_ratio()` function on the product of the prior and the likelihood. The second input for the `sd_ratio()` function is just the point at which we'd like to evaluate the Savage-Dicky ratio.

```{r}
sd_ratio(posterior, 0)
```

The output from the `sd_ratio()` function is Bayes factor object, which means that you can call `summary()` on the output object.

```{r}
summary(sd_ratio(posterior, 0))
```


## Visually comparing models

In the [basic calculations] vignette we computed Bayes factors by comparing two models. The Bayes factor told us the *relative predictive accuracy* of *two models* at a *specific point* (defined by our observation).

To compare two models, we'll first have to define them. For now, both the priors will be point priors. We'll use these priors to think through the logic, and then we change the priors to something more realistic. 

```{r}
point_0 <- prior(distribution = "point", point = 0)

point_4 <- prior(distribution = "point", point = 4)
```

Before we compare the *predictions* of the two models, let us first think through the logic of what makes a good *prediction*. 


First, let us look at the *evidence from the data*—that is, the *likelihood function*. 

```{r}
data_model <- likelihood(distribution = "normal", mean = 0, sd = 1)
plot(data_model)
```

From the plot, we can see that the data provide ample evidence that $\theta$ = 0, and far less evidence that $\theta$ = 4.

Next, let us look at the two priors. The first prior (`point_0`) indicates that, before seeing the data, we *believe*[^1] that the value of $\theta$ = 0.


```{r}
plot(data_model * point_0)

```

The second prior (`point_4`), however indicates that, before seeing the data, we *believe* that the value of $\theta$ = 4. 

```{r}
plot(data_model * point_4)
```

Now let us think about what it means for us to predict something. Specifically, what *prediction* means in terms of *change of belief*. Let us say that we believe that $\theta$ is, for example, 0, before seeing any evidence. We then observe strong evidence that $\theta$... continue later 




From this we can see that the *area under the curve* is greatest when there is a match between prior beliefs and the evidence from the data. We can use the area under the curve as a measure of *evidence for the model*. Of course, in our simple case, the model evidence is just the height of likelihood function at that specific point. 


```{r}
plot(data_model * point_0, type ="posterior")
```

```{r}
plot(data_model * point_4, type = "posterior")
```

The `bayesplay` package contains a function call `new_observation()` which allows us to update the observation in particular likelihood object. 

```{r}
model0 <- data_model * point_0
integral(model0)
```

```{r}
new_observation(model0, newdata = data.frame(mean = c(0, 1)))
```

Because these values are relative model evidence, they are not useful by themselves. Instead, one needs to compare the relative model evidence for two (or more) models. The relative model evidence at the actual observation is the *Bayes factor*, but with the `visual_compare()` function, it is possible to plot the relative model evidence for two model across the entire range of possible observation. These plots can be interpreted as the predictions *each model makes about possible observations*, with the *actual observation* indicated on the plot.

```{r}
model0 <- data_model * point_0
model4 <- data_model * point_4
visual_compare(model0, model4)
```


### Example from Rouder and Morey

```{r}
data_model <- likelihood("binomial", 7, 10)
prior_model <- prior("beta", 2.5, 1)
```

```{r}
plot(data_model)
```

```{r}
plot(prior_model)
```

```{r}
require(tidyverse)
require(bayesplay)
panel_a <- plot(data_model * prior_model, type = "pp") 
post <- data_model * prior_model
f <- function(x) post$posterior_function(x) / prior_model$prior_function(x)

f2 <- function(x) {
  data_model <- likelihood("binomial", 7, 10)
  prior_model <- prior("beta", 2.5, 1)
  post <- data_model * prior_model
f <- function(x)   (post$posterior_function(x) / prior_model$prior_function(x)) * data_model$likelihood_function(x)

   
}


require(bayesplay)
data_model <- likelihood("binomial", 7, 10)
prior_model <- prior("beta", 2.5, 1)

post <- data_model * prior_model

lik <- likelihood("binomial", 7, 10)
pri <- prior("beta", 2.5, 1)

posterior_function <- bayesplay:::calc_posterior(lik, pri)
prior_function <- pri$prior_function
updating_function <- function(x) posterior_function(x) / prior_function(x)


make_updating <- function(x, data_model, prior_model) {

  post <- data_model * prior_model
  posterior_function <- post$posterior_function
  prior_function <- prior_model$prior_function

  posterior_function(x) / prior_function(x)

}

updating_function <- purrr::partial(make_updating, 
data_model = data_model, prior_model = prior_model)

evidence_function <- (data_model * prior_model)$evidence_function

panel_b <- ggplot() + geom_function(fun = evidence_function)


posterior_function <- (data_model * prior_model)$posterior_function
likelihood_function <- data_model$likelihood_function
marginal <- function(x)  likelihood_function(x) / updating_function(x)
prediction_function <- function(x) (data_model * prior_model)$marginal(x)

pred <- data_model * prior_model

obj <- pred


panel_c <- plot(data_model) + 
  geom_function(fun = marginal)

panel_d <- ggplot() + geom_function(fun = prediction_function)
```

```{r}
panel_a + ggplot2::labs(title = "A")
```
```{r}
panel_b + ggplot2::labs(title = "B")
```

```{r}
panel_c + ggplot2::labs(title = "C")
```

```{r}
panel_d + ggplot2::labs(title = "D")
```

```{r}
prior_model <- prior(distribution = "normal", mean = 40, sd = 13.3)
data_model <- likelihood(distribution = "normal", mean = 5.5, sd = 32.35)
posterior1 <- prior_model * data_model
prior_model$prior_function(0) / posterior1$posterior_function(0)
plot(posterior1, type = "pp")
```

```{r}
prior_model <- prior(distribution = "normal", mean = 0, sd = 200)
data_model <- likelihood(distribution = "normal", mean = 5.5, sd = 32.35)
posterior2 <- prior_model * data_model
prior_model$prior_function(0) / posterior2$posterior_function(0)
plot(posterior2, type = "pp")
```

```{r}
prior_model <- prior(distribution = "normal", mean = 0, sd = 13.3)
data_model <- likelihood(distribution = "normal", mean = 5.5, sd = 32.35)
posterior3 <- prior_model * data_model
prior_model$prior_function(0) / posterior3$posterior_function(0)

plot(posterior3, type = "pp")
```

```{r}
prior_model <- prior(distribution = "normal", mean = 5.5, sd = 13.3)
data_model <- likelihood(distribution = "normal", mean = 5.5, sd = 32.35)
posterior4 <- prior_model * data_model
prior_model$prior_function(0) / posterior4$posterior_function(0)
plot(posterior3, type = "pp")
```

Something here about choosing a prior:
- We want a prior that allows us to learn from the data. 

```{r}
require(ggplot2)
ggplot() + 
  geom_function(fun = posterior1$posterior_function, colour = "blue") +
  geom_function(fun = posterior2$posterior_function, color = "red") +
  geom_function(fun = posterior3$posterior_function, color = "green") +
  geom_function(fun = posterior4$posterior_function, color = "black") +
  xlim(-100,100)
```

TODO: Include plot of 'updating factor'

- examples: very wide prior vs very narrow prior

- Something about different how different priors allow us to ask different questions. 

TODO:
Why are priors centered at 0?

Consider two situations. In the first case, we distribute our beleif about the parameters values according to the distribution shown in blue. In the second case, we distribute our beleif about the parameters values according to the distribution shown in red. 


As you'd expect, the value we assign to $\theta$ = 0 is increased. 

```{r}
require(ggplot2)
s <- 10
data_model <- likelihood(distribution = "normal", mean = 5.5 * s, sd = 32.35)
posterior1 <- prior(distribution = "normal", mean = 0, 13.3 * s) * data_model
posterior2 <- prior(distribution = "normal", mean = 5.5 * s, 13.3 * s) * data_model
ggplot() + 
  geom_function(fun = posterior1$posterior_function, colour = "blue") +
  geom_function(fun = posterior2$posterior_function, color = "red") +
  xlim(-100,100)

sd_ratio(posterior1, 0)
sd_ratio(posterior2, 0)
```

```{r}
plot(posterior1, type = "pp")
```

```{r}
plot(posterior2, type = "pp")
```


```{r}
belief_change <- function(posterior, x) {
  #posterior@prior_obj$fun(x) / posterior$posterior_function(x)
  sd_ratio(posterior, x)
}


```

```{r}
require(ggplot2)
data_model <- likelihood(distribution = "normal", mean = 5.5, sd = 32.35)
posterior1 <- prior(distribution = "normal", mean = 0, 13.3) * data_model
posterior2 <- prior(distribution = "normal", mean = 5.5, .1) * data_model
posterior3 <- prior(distribution = "normal", mean = 0, 100) * data_model

posterior1_change <- purrr::partial(belief_change, posterior1)
posterior2_change <- purrr::partial(belief_change, posterior2)
posterior3_change <- purrr::partial(belief_change, posterior3)

ggplot() + 
  geom_function(fun = posterior1_change, aes(colour = "regular")) +
  geom_function(fun = posterior2_change, aes(color = "narrow")) +
  geom_function(fun = posterior3_change, aes(color = "wide")) +
  scale_colour_manual(values = c("regular" = "green",
                                 "narrow" = "red",
                                 "wide" = "blue")) +
  xlim(0,100) + ylim(0,10)
```

```{r}
summary(sd_ratio(posterior1, 0))
```

```{r}
summary(sd_ratio(posterior3, 0))
```

```{r}
d = 2.03 / sqrt(80) # convert t to d
data_model <- likelihood(distribution = "noncentral_d", d, 79)
h0_mod <- prior(distribution = "point", point = 0)
h1_mod <- prior(distribution = "normal", mean = 0, sd = 1)
m0 <- data_model * h0_mod
m1 <- data_model * h1_mod
# TODO: Use the range from the prior -> pick the widest range
```

```{r}
visual_compare(m0, m1)
```


```{r}
visual_compare(m0, m1, type = "ratio")
```


```{r}
plot(data_model)
```

```{r}
plot(h0_mod)
```

```{r}
plot(h1_mod)
```


```{r}

f1 <- function(x){
  n = 100
  d = 2.03 / sqrt(n) # convert t to d
  data_model <- likelihood(distribution = "noncentral_d", d, n  - 1)
  h0_mod <- prior(distribution = "point", point = 0)
  h1_mod <- prior(distribution = "cauchy", location = 0, scale = 1)
  m0 <- data_model * h0_mod
  m1 <- data_model * h1_mod
  log(m1$prediction_function(x) / m0$prediction_function(x))
}

f2 <- function(x){
  n = 10
  d = 2.03 / sqrt(n) # convert t to d
  data_model <- likelihood(distribution = "noncentral_d", d, n  - 1)
  h0_mod <- prior(distribution = "point", point = 0)
  h1_mod <- prior(distribution = "cauchy", location = 0, scale = 1)
  m0 <- data_model * h0_mod
  m1 <- data_model * h1_mod
   log(m1$prediction_function(x) / m0$prediction_function(x))
}

ggplot() + 
  geom_function(fun = f1, na.rm = TRUE, color = "blue") + 
  geom_function(fun = f2, na.rm = TRUE, color = "seagreen") + 
  xlim(0, .5) +
        geom_hline(yintercept = 0, linetype = 2) + 
        geom_hline(yintercept = log(1/3), linetype = 2, color = "red") + 
        geom_hline(yintercept = log(1/10), linetype = 2, color = "red") + 
        geom_hline(yintercept = log(1/100), linetype = 2, color = "red") + 
        geom_hline(yintercept = log(3), linetype = 2, color = "blue") + 
        geom_hline(yintercept = log(10), linetype = 2, color = "blue")  +
        geom_hline(yintercept = log(100), linetype = 2, color = "blue")  +
    ylim(c(log(1/150), log(150))) +
  labs(x = "Effect size", y = "log BF")
```


[^1]: Something about about *objective* and *subjective*. 
